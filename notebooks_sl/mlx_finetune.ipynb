{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c80e0714",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cwd: /Users/ext-elias.melas/Documents/Gitcode/tomoro_finetune_cookbook/notebooks_sl\n"
          ]
        }
      ],
      "source": [
        "# Imports + environment\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "print(\"cwd:\", Path.cwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c00244e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mlx.optimizers as optim\n",
        "from mlx.utils import tree_flatten\n",
        "from mlx_lm import generate, load\n",
        "from mlx_lm.tuner import TrainingArgs, datasets, linear_to_lora_layers, train\n",
        "from transformers import PreTrainedTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "20b06fcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model\n",
        "\n",
        "# model_path = \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\"\n",
        "model_path = '../models/llama-3.1-8b/'\n",
        "model, tokenizer = load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c475d961",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "has chat_template: True\n"
          ]
        }
      ],
      "source": [
        "# Ensure tokenizer has a chat template (needed for apply_chat_template)\n",
        "# We do NOT change your selected model.\n",
        "\n",
        "# Some local tokenizers (including many llama/mistral conversions) don't ship a chat_template.\n",
        "# Provide a sensible default only if missing.\n",
        "\n",
        "def _ensure_chat_template(tok) -> None:\n",
        "    # If this is a wrapper, also try to set on the underlying tokenizer.\n",
        "    candidates = [tok]\n",
        "    inner = getattr(tok, \"tokenizer\", None) or getattr(tok, \"_tokenizer\", None)\n",
        "    if inner is not None and inner is not tok:\n",
        "        candidates.append(inner)\n",
        "\n",
        "    for t in candidates:\n",
        "        if getattr(t, \"chat_template\", None) not in (None, \"\"):\n",
        "            continue\n",
        "\n",
        "        # Llama-3-style template (works if tokenizer has the special tokens).\n",
        "        t.chat_template = (\n",
        "            \"{%- for message in messages -%}\"\n",
        "            \"{%- if loop.first -%}{{ bos_token }}{%- endif -%}\"\n",
        "            \"<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n\"\n",
        "            \"{{ message['content'] }}<|eot_id|>\"\n",
        "            \"{%- endfor -%}\"\n",
        "            \"{%- if add_generation_prompt -%}\"\n",
        "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "            \"{%- endif -%}\"\n",
        "        )\n",
        "\n",
        "        # Ensure pad token exists for batching\n",
        "        if getattr(t, \"pad_token_id\", None) is None and getattr(t, \"eos_token\", None) is not None:\n",
        "            t.pad_token = t.eos_token\n",
        "\n",
        "    # Print status from the outer object\n",
        "    ct = getattr(tok, \"chat_template\", None) or getattr(inner, \"chat_template\", None)\n",
        "    print(\"has chat_template:\", bool(ct))\n",
        "\n",
        "\n",
        "_ensure_chat_template(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "48fb00b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "adapter_path = \"../adapters/local_lora_llama_3.1_8b\"\n",
        "os.makedirs(adapter_path, exist_ok=True)\n",
        "adapter_config_path = os.path.join(adapter_path, \"adapter_config.json\")\n",
        "adapter_file_path = os.path.join(adapter_path, \"adapters.safetensors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b41252f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# these can be in a yaml file instead\n",
        "\n",
        "lora_config = {\n",
        "    \"num_layers\": 8,\n",
        "    \"lora_parameters\": {\n",
        "        \"rank\": 8,\n",
        "        \"scale\": 20.0,\n",
        "        \"dropout\": 0.0,\n",
        "    },\n",
        "}\n",
        "\n",
        "# which we save into adapter_path\n",
        "with open(adapter_config_path, \"w\") as f:\n",
        "    json.dump(lora_config, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ee2c4d77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can also set our training params\n",
        "training_args = TrainingArgs(\n",
        "    adapter_file=adapter_file_path,\n",
        "    iters=200,\n",
        "    steps_per_eval=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a2643b",
      "metadata": {},
      "source": [
        "- In the LoRA framework, most of the model’s original parameters remain unchanged during fine-tuning. \n",
        "- The model.freeze() command is used to set these parameters to a non-trainable state so that their weights aren’t updated during backpropagation. This way, only the newly introduced low-rank adaptation matrices (LoRA parameters) are optimized, reducing computational overhead and memory usage while preserving the original model’s knowledge.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb6bda9",
      "metadata": {},
      "source": [
        "\n",
        "- The linear_to_lora_layers function converts or wraps some of the model’s linear layers into LoRA layers. Essentially, it replaces (or augments) selected linear layers with their LoRA counterparts, which include the additional low-rank matrices that will be trained. \n",
        "- The configuration parameters (like the number of layers and specific LoRA parameters) determine which layers are modified and how the LoRA adapters are set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d724edbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 5242880\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers.0): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.1): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.2): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.3): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.4): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.5): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.6): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.7): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.8): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.9): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.10): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.11): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.12): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.13): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.14): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.15): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.16): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.17): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.18): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.19): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.20): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.21): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.22): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.23): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (k_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (v_proj): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "        (down_proj): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "        (up_proj): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.24): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.25): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.26): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.27): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.28): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.29): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.30): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (layers.31): TransformerBlock(\n",
              "      (self_attn): Attention(\n",
              "        (q_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (k_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (v_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=1024, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (o_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (rope): Llama3RoPE()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (gate_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (down_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=14336, output_dims=4096, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "        (up_proj): LoRALinear(\n",
              "          (linear): Linear(input_dims=4096, output_dims=14336, bias=False)\n",
              "          (dropout): Dropout(p=0.0)\n",
              "        )\n",
              "      )\n",
              "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
              "    )\n",
              "    (norm): RMSNorm(4096, eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(input_dims=4096, output_dims=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we should also verify that only a small subset of parameters are set for training\n",
        "# and activate training mode while freezing the main model params\n",
        "\n",
        "model.freeze()\n",
        "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
        "num_train_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
        "print(f\"Number of trainable parameters: {num_train_params}\")\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "112b5dbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# class to follow the training progress\n",
        "class Metrics:\n",
        "    def __init__(self) -> None:\n",
        "        self.train_losses: List[Tuple[int, float]] = []\n",
        "        self.val_losses: List[Tuple[int, float]] = []\n",
        "\n",
        "    def on_train_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
        "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
        "\n",
        "    def on_val_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
        "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
        "\n",
        "metrics = Metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c4cff082",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset dir: /Users/ext-elias.melas/Documents/Gitcode/tomoro_finetune_cookbook/data/mlx_lora_ar\n",
            "train: /Users/ext-elias.melas/Documents/Gitcode/tomoro_finetune_cookbook/data/mlx_lora_ar/train.jsonl\n",
            "valid: /Users/ext-elias.melas/Documents/Gitcode/tomoro_finetune_cookbook/data/mlx_lora_ar/valid.jsonl\n",
            "train examples: 2243\n",
            "valid examples: 249\n",
            "first example tokens: 1129 offset: 832\n"
          ]
        }
      ],
      "source": [
        "# Load MLX-LM local dataset (train.jsonl + valid.jsonl)\n",
        "\n",
        "import types\n",
        "\n",
        "# Repo root discovery (so paths work regardless of cwd)\n",
        "REPO_ROOT = Path.cwd().resolve().parent if (Path.cwd().name == \"notebooks_sl\") else Path.cwd().resolve()\n",
        "if not (REPO_ROOT / \"data\").exists():\n",
        "    for p in [Path.cwd().resolve(), *Path.cwd().resolve().parents]:\n",
        "        if (p / \"data\").exists():\n",
        "            REPO_ROOT = p\n",
        "            break\n",
        "\n",
        "MLX_DATA_DIR = REPO_ROOT / \"data\" / \"mlx_lora_ar\"\n",
        "train_path = MLX_DATA_DIR / \"train.jsonl\"\n",
        "valid_path = MLX_DATA_DIR / \"valid.jsonl\"\n",
        "\n",
        "print(\"dataset dir:\", MLX_DATA_DIR)\n",
        "print(\"train:\", train_path)\n",
        "print(\"valid:\", valid_path)\n",
        "\n",
        "if not train_path.exists() or not valid_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Missing train/valid JSONL. Run notebooks_sl/build_dataset_mlx.ipynb to generate: \"\n",
        "        f\"{train_path} and {valid_path}\"\n",
        "    )\n",
        "\n",
        "# IMPORTANT: do NOT unwrap to the underlying transformers tokenizer here.\n",
        "# The MLX TokenizerWrapper returned by `mlx_lm.load()` can implement chat templating\n",
        "# even when the HF tokenizer has no `chat_template` set.\n",
        "base_tokenizer = tokenizer\n",
        "\n",
        "# Config controls which keys to read and whether to mask the prompt.\n",
        "config = types.SimpleNamespace(\n",
        "    chat_feature=\"messages\",\n",
        "    mask_prompt=True,\n",
        ")\n",
        "\n",
        "train_dataset, valid_dataset, _test_dataset = datasets.load_local_dataset(\n",
        "    MLX_DATA_DIR,\n",
        "    base_tokenizer,\n",
        "    config,\n",
        ")\n",
        "\n",
        "# trainer.iterate_batches expects dataset[idx] to return (tokens, offset).\n",
        "train_dataset = datasets.CacheDataset(train_dataset)\n",
        "valid_dataset = datasets.CacheDataset(valid_dataset)\n",
        "\n",
        "print(\"train examples:\", len(train_dataset))\n",
        "print(\"valid examples:\", len(valid_dataset))\n",
        "\n",
        "# Force tokenization of the first element as a sanity check\n",
        "first_tokens, first_offset = train_dataset[0]\n",
        "print(\"first example tokens:\", len(first_tokens), \"offset:\", first_offset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ee76316c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train() accepts: ['args', 'iterate_batches', 'loss', 'model', 'optimizer', 'train_dataset', 'training_callback', 'val_dataset']\n",
            "passing: ['args', 'model', 'optimizer', 'train_dataset', 'training_callback', 'val_dataset']\n",
            "dropping unsupported kwargs: ['tokenizer']\n",
            "Starting training..., iters: 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Calculating loss...:  52%|█████▏    | 13/25 [03:43<04:12, 21.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2406 will be truncated to 2048. Consider pre-splitting your data to save memory.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...:  88%|████████▊ | 22/25 [06:05<00:45, 15.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2258 will be truncated to 2048. Consider pre-splitting your data to save memory.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating loss...: 100%|██████████| 25/25 [06:54<00:00, 16.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 1: Val loss 2.858, Val took 414.937s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Kick off training using the loaded train/valid datasets\n",
        "# NOTE: different mlx-lm versions expose different `train()` signatures.\n",
        "# We keep your preferred kwargs, but filter out any unsupported ones.\n",
        "\n",
        "import inspect\n",
        "\n",
        "train_set = train_dataset\n",
        "val_set = valid_dataset\n",
        "\n",
        "requested_kwargs = {\n",
        "    \"model\": model,\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"args\": training_args,\n",
        "    \"optimizer\": optim.Adam(learning_rate=1e-5),\n",
        "    \"train_dataset\": train_set,\n",
        "    \"val_dataset\": val_set,\n",
        "    \"training_callback\": metrics,\n",
        "}\n",
        "\n",
        "sig = inspect.signature(train)\n",
        "accepted = set(sig.parameters.keys())\n",
        "kwargs = {k: v for k, v in requested_kwargs.items() if k in accepted}\n",
        "dropped = [k for k in requested_kwargs.keys() if k not in accepted]\n",
        "\n",
        "print(\"train() accepts:\", sorted(accepted))\n",
        "print(\"passing:\", sorted(kwargs.keys()))\n",
        "if dropped:\n",
        "    print(\"dropping unsupported kwargs:\", dropped)\n",
        "\n",
        "train(**kwargs)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
