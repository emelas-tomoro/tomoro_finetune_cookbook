{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports + environment\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "from enum import StrEnum\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cd06a863",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /Users/ext-elias.melas/.cache/kagglehub/datasets/naserabdullahalam/phishing-email-dataset/versions/1\n",
            "Files in dataset: ['Nigerian_Fraud.csv', 'Ling.csv', 'Nazario.csv', 'SpamAssasin.csv', 'CEAS_08.csv', 'phishing_email.csv', 'Enron.csv']\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n",
        "files = os.listdir(path)\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "print(\"Files in dataset:\", files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2cc5d8e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_data = pd.read_csv(f'{path}/Enron.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ee8c75c3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "0    15791\n",
              "1    13976\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lora_data['label'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c69e5edf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    2241\n",
              "0    2231\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    560\n",
              "0    558\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Balance positives/negatives and split into train/test (for evaluation).\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "true_df = lora_data[lora_data[\"label\"] == True]\n",
        "false_df = lora_data[lora_data[\"label\"] == False]\n",
        "\n",
        "false_down = false_df.sample(n=len(true_df), random_state=42)\n",
        "lora_data_balanced = (\n",
        "    pd.concat([true_df, false_down]).sample(frac=0.2, random_state=42).reset_index(drop=True)\n",
        ")\n",
        "\n",
        "lora_train, lora_test = train_test_split(\n",
        "    lora_data_balanced,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=lora_data_balanced[\"label\"],\n",
        ")\n",
        "\n",
        "display(lora_train[\"label\"].value_counts(dropna=False))\n",
        "display(lora_test[\"label\"].value_counts(dropna=False))\n",
        "\n",
        "lora_train.to_parquet(\"../data/finetuning/lora_train_emails.parquet\")\n",
        "lora_test.to_parquet(\"../data/finetuning/lora_test_emails.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9c08cf25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared examples: 4472\n",
            "WRITE_TRAINING_DATA is False; not writing any files.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# SAFE-BY-DEFAULT: this cell does NOT write any files unless you explicitly opt in.\n",
        "WRITE_TRAINING_DATA = False\n",
        "OVERWRITE_OUTPUTS = False\n",
        "RUN_TAG = 'spam'\n",
        "\n",
        "system_prompt = Path(\"../data/verification/system_prompt_spam.md\").read_text()\n",
        "\n",
        "ft_training_data_list: list[dict[str, str]] = []\n",
        "for _, d in lora_train.iterrows():\n",
        "    subject = d[\"subject\"]\n",
        "    body = d[\"body\"]\n",
        "    is_spam = d[\"label\"]\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "Email subject:\n",
        "---------------------------------------------------\n",
        "\\n{subject}\\n\\n\n",
        "---------------------------------------------------\n",
        "Email body:\n",
        "---------------------------------------------------\n",
        "\\n{body}\\n\\n\n",
        "---------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    ft_training_data_list.append(\n",
        "        {\n",
        "            \"instruction\": system_prompt,\n",
        "            \"input\": user_prompt,\n",
        "            # Keep the same output format your Tinker model expects.\n",
        "            \"output\": f\"is_spam: {is_spam}\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Prepared examples:\", len(ft_training_data_list))\n",
        "\n",
        "# If you want to write, write to a VERSIONED path by default.\n",
        "out_path = Path(f\"../data/finetuning/training_data_{RUN_TAG}.json\")\n",
        "\n",
        "if WRITE_TRAINING_DATA:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if out_path.exists() and not OVERWRITE_OUTPUTS:\n",
        "        raise FileExistsError(\n",
        "            f\"Refusing to overwrite existing file: {out_path}. Set OVERWRITE_OUTPUTS=True to overwrite.\"\n",
        "        )\n",
        "\n",
        "    import json\n",
        "\n",
        "    out_path.write_text(json.dumps(ft_training_data_list, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "    print(\"Wrote:\", out_path)\n",
        "else:\n",
        "    print(\"WRITE_TRAINING_DATA is False; not writing any files.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
