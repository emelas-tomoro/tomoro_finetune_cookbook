{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08ac1f62",
      "metadata": {},
      "source": [
        "### Local SFT (8-bit + LoRA) — Hugging Face Transformers\n",
        "\n",
        "This notebook mirrors `notebooks_sl/tinker_finetune.ipynb`, but trains **locally** using:\n",
        "- `transformers` + `peft`\n",
        "- **8-bit quantization** via `bitsandbytes`\n",
        "- LoRA adapters (saved under `adapters/`)\n",
        "\n",
        "It consumes the same conversation JSONL format:\n",
        "\n",
        "```json\n",
        "{\"messages\": [{\"role\":\"system\",\"content\":\"...\"}, {\"role\":\"user\",\"content\":\"...\"}, {\"role\":\"assistant\",\"content\":\"...\"}]}\n",
        "```\n",
        "\n",
        "Expected inputs in this repo:\n",
        "- `data/finetuning/training_data_spam.json` (instruction-tuning JSON array)\n",
        "- `data/finetuning/tinker_conversations_spam.jsonl` (conversation JSONL)\n",
        "\n",
        "Outputs:\n",
        "- LoRA adapter: `adapters/local_lora_8bit_spam/`\n",
        "- Optional: training logs under `logs/local_lora_8bit_spam/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeec62eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config — match notebooks_sl/tinker_finetune.ipynb defaults\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "AR_FINETUNE_ROOT = Path.cwd().resolve().parent if (Path.cwd().name == \"notebooks_sl\") else Path.cwd().resolve()\n",
        "# If launched from repo root, AR_FINETUNE_ROOT is already correct.\n",
        "if not (AR_FINETUNE_ROOT / \"data\" / \"finetuning\").exists():\n",
        "    # If launched from within notebooks/ or other subdir, walk upward.\n",
        "    for p in [Path.cwd().resolve(), *Path.cwd().resolve().parents]:\n",
        "        if (p / \"data\" / \"finetuning\").exists():\n",
        "            AR_FINETUNE_ROOT = p\n",
        "            break\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
        "LOCAL_MODEL_DIR = AR_FINETUNE_ROOT / \"models\" / \"llama-3.1-8b\"\n",
        "\n",
        "IN_PATH = AR_FINETUNE_ROOT / \"data\" / \"finetuning\" / \"training_data_spam.json\"\n",
        "CONVERSATIONS_JSONL = AR_FINETUNE_ROOT / \"data\" / \"finetuning\" / \"tinker_conversations_spam.jsonl\"\n",
        "\n",
        "OUTPUT_ADAPTER_DIR = AR_FINETUNE_ROOT / \"adapters\" / \"local_lora_8bit_spam\"\n",
        "LOG_DIR = AR_FINETUNE_ROOT / \"logs\" / \"local_lora_8bit_spam\"\n",
        "\n",
        "# Training hyperparams (keep close to Tinker defaults; adjust for your GPU)\n",
        "MAX_LENGTH = 8192\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "# These are *local* knobs (Tinker had batch_size=16 with remote GPU)\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 16\n",
        "\n",
        "# LoRA hyperparams\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# A small run is useful to validate end-to-end quickly\n",
        "LIMIT_TRAIN_EXAMPLES: int | None = None  # e.g. 256\n",
        "\n",
        "print(\"ar_finetune root:\", AR_FINETUNE_ROOT)\n",
        "print(\"local model dir:\", LOCAL_MODEL_DIR)\n",
        "print(\"input:\", IN_PATH)\n",
        "print(\"conversations:\", CONVERSATIONS_JSONL)\n",
        "print(\"output adapter:\", OUTPUT_ADAPTER_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6613ae1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment check\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
        "    print(\"bf16 supported:\", torch.cuda.is_bf16_supported())\n",
        "\n",
        "# Helpful default to reduce fragmentation on long-context runs\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8735587e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1 — Ensure conversation JSONL exists (same format as Tinker)\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "def ensure_conversations_jsonl(\n",
        "    *,\n",
        "    in_path: Path,\n",
        "    out_path: Path,\n",
        "    overwrite: bool = False,\n",
        ") -> None:\n",
        "    if out_path.exists() and not overwrite:\n",
        "        print(\"Using existing:\", out_path)\n",
        "        return\n",
        "\n",
        "    data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
        "    if not isinstance(data, list):\n",
        "        raise TypeError(f\"Expected a JSON array at {in_path}, got {type(data).__name__}\")\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for ex in data:\n",
        "            instruction = (ex.get(\"instruction\") or \"\").strip()\n",
        "            user_input = (ex.get(\"input\") or \"\").strip()\n",
        "            output = (ex.get(\"output\") or \"\").strip()\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": instruction},\n",
        "                {\"role\": \"user\", \"content\": user_input},\n",
        "                {\"role\": \"assistant\", \"content\": output},\n",
        "            ]\n",
        "            f.write(json.dumps({\"messages\": messages}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(\"Wrote:\", out_path)\n",
        "    print(\"Head:\")\n",
        "    print(out_path.open(\"r\", encoding=\"utf-8\").read().splitlines()[0][:500])\n",
        "\n",
        "\n",
        "ensure_conversations_jsonl(in_path=IN_PATH, out_path=CONVERSATIONS_JSONL, overwrite=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e82f690",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 — Load the dataset (conversation JSONL)\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"train\": str(CONVERSATIONS_JSONL)},\n",
        "    split=\"train\",\n",
        ")\n",
        "\n",
        "print(raw)\n",
        "print(raw[0].keys())\n",
        "print(raw[0][\"messages\"][0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1eee0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 — Tokenizer + assistant-only label masking\n",
        "\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\n",
        "    str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "    use_fast=True,\n",
        ")\n",
        "\n",
        "# Some local tokenizers don't ship a chat template; set a reasonable Llama-3-style default.\n",
        "# This matches the common Meta Llama 3 instruct formatting:\n",
        "#   <|begin_of_text|><|start_header_id|>role<|end_header_id|>\\n\\ncontent<|eot_id|> ...\n",
        "if getattr(tok, \"chat_template\", None) in (None, \"\"):\n",
        "    tok.chat_template = (\n",
        "        \"{%- for message in messages -%}\"\n",
        "        \"{%- if loop.first -%}{{ bos_token }}{%- endif -%}\"\n",
        "        \"<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n\"\n",
        "        \"{{ message['content'] }}<|eot_id|>\"\n",
        "        \"{%- endfor -%}\"\n",
        "        \"{%- if add_generation_prompt -%}\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        \"{%- endif -%}\"\n",
        "    )\n",
        "\n",
        "# Llama-family models typically need an explicit pad token for batching.\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "print(\"has chat_template:\", bool(getattr(tok, \"chat_template\", None)))\n",
        "print(\"pad_token_id:\", tok.pad_token_id)\n",
        "print(\"eos_token_id:\", tok.eos_token_id)\n",
        "\n",
        "\n",
        "def encode_chat_with_assistant_labels(\n",
        "    messages: List[Dict[str, str]],\n",
        "    *,\n",
        "    max_length: int,\n",
        ") -> Dict[str, List[int]]:\n",
        "    \"\"\"Build (input_ids, attention_mask, labels) where loss is only on assistant content.\n",
        "\n",
        "    Strategy:\n",
        "    - Use the model's chat template to tokenize the full conversation.\n",
        "    - Re-tokenize incremental prefixes to locate which token ranges correspond to each message.\n",
        "    - Mark assistant message ranges as trainable (labels = token_id), everything else = -100.\n",
        "\n",
        "    Notes:\n",
        "    - This is intentionally simple and robust; it trades speed for clarity.\n",
        "    - For large datasets you may want to cache prefix tokenization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Full tokenization\n",
        "    full_ids: List[int] = tok.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "\n",
        "    # Build a boolean mask aligned to full_ids\n",
        "    train_mask = [False] * len(full_ids)\n",
        "\n",
        "    prev_prefix: List[Dict[str, str]] = []\n",
        "    # Some tokenizer implementations error on apply_chat_template([]); treat empty prefix as 0 tokens.\n",
        "    prev_ids: List[int] = []\n",
        "\n",
        "    for m in messages:\n",
        "        cur_prefix = [*prev_prefix, m]\n",
        "        cur_ids: List[int] = tok.apply_chat_template(\n",
        "            cur_prefix,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "\n",
        "        start = len(prev_ids)\n",
        "        end = len(cur_ids)\n",
        "\n",
        "        if m.get(\"role\") == \"assistant\":\n",
        "            for i in range(start, end):\n",
        "                train_mask[i] = True\n",
        "\n",
        "        prev_prefix = cur_prefix\n",
        "        prev_ids = cur_ids\n",
        "\n",
        "    # Truncate\n",
        "    input_ids = full_ids[:max_length]\n",
        "    train_mask = train_mask[:max_length]\n",
        "\n",
        "    labels = [tid if train_mask[i] else -100 for i, tid in enumerate(input_ids)]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "\n",
        "# Quick sanity check on 1 example\n",
        "ex0 = raw[0][\"messages\"]\n",
        "enc0 = encode_chat_with_assistant_labels(ex0, max_length=512)\n",
        "print({k: len(v) for k, v in enc0.items()})\n",
        "print(\"trainable tokens:\", sum(1 for x in enc0[\"labels\"] if x != -100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e439cee3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4 — Build tokenized train dataset\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def to_features(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    return encode_chat_with_assistant_labels(ex[\"messages\"], max_length=MAX_LENGTH)\n",
        "\n",
        "train_ds = raw\n",
        "if LIMIT_TRAIN_EXAMPLES is not None:\n",
        "    # deterministic sample for quick iteration\n",
        "    idxs = list(range(len(train_ds)))\n",
        "    random.Random(0).shuffle(idxs)\n",
        "    idxs = idxs[:LIMIT_TRAIN_EXAMPLES]\n",
        "    train_ds = train_ds.select(idxs)\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    to_features,\n",
        "    remove_columns=train_ds.column_names,\n",
        "    desc=\"Tokenizing + building assistant-only labels\",\n",
        ")\n",
        "\n",
        "print(train_ds)\n",
        "print(train_ds[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b257bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5 — Load base model in 8-bit and attach LoRA\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Backend selection\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "USE_MPS = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    print(\"Using CUDA (8-bit bitsandbytes + LoRA)\")\n",
        "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "    # Train on a single GPU to keep `Trainer` happy (avoid sharding/device_map auto).\n",
        "    # If you truly need sharding, switch to an `accelerate` training script instead of `Trainer`.\n",
        "    device_map = {\"\": 0}\n",
        "\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=dtype,\n",
        "    )\n",
        "\n",
        "    base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "elif USE_MPS:\n",
        "    # bitsandbytes 8-bit quantization requires NVIDIA CUDA.\n",
        "    # On Apple Silicon, we run LoRA in fp16 on MPS.\n",
        "    print(\"Using MPS (fp16 + LoRA; no bitsandbytes)\")\n",
        "    bnb_config = None\n",
        "    device_map = None\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    base_model.to(torch.device(\"mps\"))\n",
        "\n",
        "else:\n",
        "    print(\"Using CPU (fp32 + LoRA; slow, no bitsandbytes)\")\n",
        "    bnb_config = None\n",
        "    device_map = None\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "        torch_dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "base_model.gradient_checkpointing_enable()\n",
        "\n",
        "# Llama-style target modules (covers common linear projections)\n",
        "lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=lora_target_modules,\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f546a787",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6 — Trainer setup (simple SFT)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLMAssistantOnly:\n",
        "    pad_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        # Pad input_ids/attention_mask to max length in batch; pad labels with -100.\n",
        "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        labels = []\n",
        "        for f in features:\n",
        "            n = len(f[\"input_ids\"])\n",
        "            pad = max_len - n\n",
        "            input_ids.append(f[\"input_ids\"] + [self.pad_token_id] * pad)\n",
        "            attention_mask.append(f[\"attention_mask\"] + [0] * pad)\n",
        "            labels.append(f[\"labels\"] + [-100] * pad)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "collator = DataCollatorForCausalLMAssistantOnly(pad_token_id=tok.pad_token_id)\n",
        "\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_ADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "optim_name = \"paged_adamw_8bit\" if torch.cuda.is_available() else \"adamw_torch\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOG_DIR),\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.03,\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=[],\n",
        "    optim=optim_name,\n",
        "    bf16=bool(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
        "    fp16=bool(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()) or (hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()),\n",
        "    # Use Apple Silicon GPU when available\n",
        "    use_mps_device=bool(hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()),\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"Trainer optim:\", optim_name)\n",
        "print(\"use_mps_device:\", getattr(args, \"use_mps_device\", None))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"Ready to train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf6ab19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7 — Train + save adapter\n",
        "\n",
        "train_result = trainer.train()\n",
        "print(train_result)\n",
        "\n",
        "# Save LoRA adapter weights + config\n",
        "model.save_pretrained(str(OUTPUT_ADAPTER_DIR))\n",
        "tok.save_pretrained(str(OUTPUT_ADAPTER_DIR))\n",
        "\n",
        "print(\"Saved adapter to:\", OUTPUT_ADAPTER_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc976a1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8 — Quick local inference smoke test\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "# Reload base model (8-bit) and attach saved adapter for clean inference\n",
        "# Reload base model and attach saved adapter for clean inference.\n",
        "# - CUDA: 8-bit bitsandbytes\n",
        "# - MPS: fp16\n",
        "# - CPU: fp32\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    infer_base = AutoModelForCausalLM.from_pretrained(\n",
        "        str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": 0},\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    )\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    infer_base = AutoModelForCausalLM.from_pretrained(\n",
        "        str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "    ).to(torch.device(\"mps\"))\n",
        "else:\n",
        "    infer_base = AutoModelForCausalLM.from_pretrained(\n",
        "        str(LOCAL_MODEL_DIR) if LOCAL_MODEL_DIR.exists() else MODEL_NAME,\n",
        "        torch_dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "infer_model = PeftModel.from_pretrained(infer_base, str(OUTPUT_ADAPTER_DIR))\n",
        "infer_model.eval()\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an email security analyst. Return exactly one line: is_spam: 0/1/-1\"},\n",
        "    {\"role\": \"user\", \"content\": \"Subject: Reset your password\\nBody: Click here to reset your password immediately...\"},\n",
        "]\n",
        "\n",
        "prompt_ids = tok.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "\n",
        "# When using `device_map=\"auto\"`, models can be sharded and `model.device` may be unset.\n",
        "# Put inputs on the embedding device (or fall back to first parameter device).\n",
        "def _infer_input_device(m) -> torch.device:\n",
        "    dm = getattr(m, \"hf_device_map\", None)\n",
        "    if isinstance(dm, dict) and dm:\n",
        "        for key in (\n",
        "            \"model.embed_tokens\",\n",
        "            \"model.model.embed_tokens\",\n",
        "            \"transformer.wte\",\n",
        "            \"embed_tokens\",\n",
        "        ):\n",
        "            if key in dm:\n",
        "                return torch.device(dm[key])\n",
        "        return torch.device(next(iter(dm.values())))\n",
        "    return next(m.parameters()).device\n",
        "\n",
        "input_device = _infer_input_device(infer_model)\n",
        "input_ids = torch.tensor([prompt_ids], device=input_device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = infer_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        do_sample=False,\n",
        "        temperature=1.0,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "    )\n",
        "\n",
        "gen = tok.decode(out[0][len(prompt_ids):], skip_special_tokens=True)\n",
        "print(gen.strip())\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
