{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3e01c35",
      "metadata": {},
      "source": [
        "### Tinker phishing classifier (clean)\n",
        "\n",
        "This notebook runs **inference** with your fine-tuned Tinker sampler and produces a final DataFrame that includes:\n",
        "- `category`: the model prediction (`GENUINE`/`PHISHING`/`ASSIGN_TO_AGENT`)\n",
        "- `agent_notes_pred`: the model-provided agent notes extracted from the assistant text (e.g. `agent_notes: ...`)\n",
        "\n",
        "**Prereqs**\n",
        "- `TINKER_API_KEY` is set (this notebook loads `.env` if present).\n",
        "- You have a completed run log with `checkpoints.jsonl` at `LOG_PATH` (default: `/tmp/tinker-examples/sl_ar_phishing`).\n",
        "\n",
        "**Outputs**\n",
        "- `df_tinker`: per-ticket predictions + `agent_notes_pred`\n",
        "- `df_phishing_messages_tinker`: your evaluation DataFrame with predictions merged in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ext-elias.melas/Documents/Gitcode/ar_finetune/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports + environment\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "from enum import StrEnum\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Build the evaluation dataset\n",
        "\n",
        "This section reproduces the minimal data prep needed to create `lora_test` with these columns:\n",
        "- `ticket_id`\n",
        "- `messages_player`\n",
        "- `ticket_tags_with_description`\n",
        "- `features`\n",
        "- `ml_features`\n",
        "- `is_phishing` (label used for evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lambda_sort_by_message_order(message_array: str) -> list[dict[str, Any]]:\n",
        "    # `message_map_list` is a JSON-encoded list of messages with `message_order`.\n",
        "    data = json.loads(message_array)\n",
        "    return sorted(data, key=lambda obj: int(obj[\"message_order\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(139006, 44)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load raw messages + AR features (paths match the original notebook).\n",
        "\n",
        "df_messages = pd.read_csv(\"../data/verification/failed_hs_bots_agents_messages_30d_all_games.csv\")\n",
        "df_ar_results = pd.read_csv(\"../data/verification/ar_sep_2025.csv\")\n",
        "\n",
        "# Parse the message list and build helper fields used downstream.\n",
        "df_messages[\"messages_full\"] = df_messages[\"message_map_list\"].apply(lambda_sort_by_message_order)\n",
        "df_messages[\"messages_player_only\"] = df_messages[\"messages_full\"].apply(\n",
        "    lambda x: \",\\n\".join(\n",
        "        f\"{i}. {m['body']}\"\n",
        "        for i, m in enumerate((m for m in x if m.get(\"role\") == \"player\"), start=1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# These are *ticket* agent notes extracted from the raw message map.\n",
        "# (We keep them, but note: the model-predicted notes will be stored separately as `agent_notes_pred`.)\n",
        "df_messages[\"agent_notes_ticket\"] = df_messages[\"messages_full\"].apply(\n",
        "    lambda x: \"\\n\".join(\n",
        "        f\"{i}. {m['body']}\"\n",
        "        for i, m in enumerate(\n",
        "            (\n",
        "                m\n",
        "                for m in x\n",
        "                if m.get(\"message_type\") == \"add_private_note\" and m.get(\"role\") == \"agent\"\n",
        "            ),\n",
        "            start=1,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# Outer merge to preserve rows even if one side is missing.\n",
        "df_models_messages_features = df_messages.merge(df_ar_results, on=\"ticket_id\", how=\"outer\")\n",
        "df_models_messages_features.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(139006, 44)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Keep only rows where AR produced feature text.\n",
        "# `features_messages` is expected to come from `ar_sep_2025.csv`.\n",
        "df_models_messages_features = df_models_messages_features.loc[\n",
        "    ~df_models_messages_features[\"message_map_list\"].isna()\n",
        "]\n",
        "df_models_messages_features.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0c/96px_5vs3wj8lpnj4_mzxvxm0000gq/T/ipykernel_79793/714720921.py:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_models_messages_features.apply(get_phishing_attempts_either, axis=1).fillna(False)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "phishing_attempts_either\n",
              "False    137138\n",
              "True       1868\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_phishing_attempts_either(row: pd.Series) -> bool | None:\n",
        "    # Label is True if either the agent label or bot label says phishing.\n",
        "    if row.get(\"phishing_attempts_agent\") == 1 or row.get(\"phishing_attempts_bot\") == 1:\n",
        "        return True\n",
        "    return None\n",
        "\n",
        "df_models_messages_features[\"phishing_attempts_either\"] = (\n",
        "    df_models_messages_features.apply(get_phishing_attempts_either, axis=1).fillna(False)\n",
        ")\n",
        "df_models_messages_features[\"phishing_attempts_either\"].value_counts(dropna=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.317180553357409)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "np.float64(0.44865689250823704)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "np.float64(0.4779793677970735)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Parse the AR blob embedded in messages (same helpers as original notebook).\n",
        "\n",
        "def get_features_from_messages(x: list[dict[str, Any]]) -> str | None:\n",
        "    substring = \"Risk assessment results for malicious_account_recovery:\"\n",
        "    try:\n",
        "        for m in x:\n",
        "            body = m.get(\"body\", \"\")\n",
        "            if \"Risk assessment results\" in body:\n",
        "                return body.split(substring)[1]\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_ml_result_from_messages(x: list[dict[str, Any]]) -> str | None:\n",
        "    pattern = r\"account_recovery_eligibility:\\s*([A-Z_]+)(?=\\s*âœ…?\\s*SuspiciousSessionsRule)\"\n",
        "    try:\n",
        "        for m in x:\n",
        "            body = m.get(\"body\", \"\")\n",
        "            if \"SuspiciousSessionsRule\" in body:\n",
        "                m_ = re.search(pattern, body)\n",
        "                return m_.group(1) if m_ else None\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_ml_features_from_messages(x: list[dict[str, Any]]) -> str | None:\n",
        "    pattern = r\"SuspiciousSessionsRule[\\s\\S]*?(?=\\s+Compare Tab:)\"\n",
        "    try:\n",
        "        for m in x:\n",
        "            body = m.get(\"body\", \"\")\n",
        "            if \"SuspiciousSessionsRule\" in body:\n",
        "                m_ = re.search(pattern, body)\n",
        "                return m_.group(0) if m_ else None\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "df_models_messages_features[\"features_messages\"] = df_models_messages_features[\"messages_full\"].apply(get_features_from_messages)\n",
        "df_models_messages_features[\"ml_features_messages\"] = df_models_messages_features[\"messages_full\"].apply(get_ml_features_from_messages)\n",
        "df_models_messages_features[\"ml_result_messages\"] = df_models_messages_features[\"messages_full\"].apply(get_ml_result_from_messages)\n",
        "\n",
        "display(\n",
        "    df_models_messages_features[\"features_messages\"].isna().mean(),\n",
        "    df_models_messages_features[\"ml_features_messages\"].isna().mean(),\n",
        "    df_models_messages_features[\"ml_result_messages\"].isna().mean(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4c06a035",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76640, 48)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_models_messages_features = df_models_messages_features.loc[\n",
        "    ~df_models_messages_features[\"ml_features_messages\"].isna()\n",
        "]\n",
        "df_models_messages_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76640, 9)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build the dataset used for evaluation/inference.\n",
        "\n",
        "training_data_list: list[dict[str, Any]] = []\n",
        "for _, row in df_models_messages_features.iterrows():\n",
        "    training_data_list.append(\n",
        "        {\n",
        "            \"messages_full\": row.get(\"messages_full\"),\n",
        "            \"ticket_id\": row.get(\"ticket_id\"),\n",
        "            \"messages_player\": row.get(\"messages_player_only\"),\n",
        "            \"ticket_tags_with_description\": row.get(\"ticket_tags_with_description\"),\n",
        "            \"is_phishing\": row.get(\"phishing_attempts_either\"),\n",
        "            \"features\": row.get(\"features_messages\"),\n",
        "            \"ml_features\": row.get(\"ml_features_messages\"),\n",
        "            \"ml_result\": row.get(\"ml_result_messages\"),\n",
        "            \"agent_notes\": row.get(\"agent_notes\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "lora_data = pd.DataFrame(training_data_list)\n",
        "lora_data.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "is_phishing\n",
              "True     1246\n",
              "False    1246\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "is_phishing\n",
              "True     312\n",
              "False    312\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Balance positives/negatives and split into train/test (for evaluation).\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "true_df = lora_data[lora_data[\"is_phishing\"] == True]\n",
        "false_df = lora_data[lora_data[\"is_phishing\"] == False]\n",
        "\n",
        "false_down = false_df.sample(n=len(true_df), random_state=42)\n",
        "lora_data_balanced = (\n",
        "    pd.concat([true_df, false_down]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        ")\n",
        "\n",
        "lora_train, lora_test = train_test_split(\n",
        "    lora_data_balanced,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=lora_data_balanced[\"is_phishing\"],\n",
        ")\n",
        "\n",
        "display(lora_train[\"is_phishing\"].value_counts(dropna=False))\n",
        "display(lora_test[\"is_phishing\"].value_counts(dropna=False))\n",
        "\n",
        "lora_train.to_parquet(\"../data/finetuning/lora_train.parquet\")\n",
        "lora_test.to_parquet(\"../data/finetuning/lora_test.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1c2e94",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared examples: 2492\n",
            "WRITE_TRAINING_DATA is False; not writing any files.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# SAFE-BY-DEFAULT: this cell does NOT write any files unless you explicitly opt in.\n",
        "WRITE_TRAINING_DATA = False\n",
        "OVERWRITE_OUTPUTS = False\n",
        "RUN_TAG = 'n_10'\n",
        "\n",
        "system_prompt = Path(\"../data/verification/system_prompt.md\").read_text()\n",
        "\n",
        "ft_training_data_list: list[dict[str, str]] = []\n",
        "for _, d in lora_train.iterrows():\n",
        "    messages = d[\"messages_player\"]\n",
        "    features = d[\"features\"]\n",
        "    ml_features = d[\"ml_features\"]\n",
        "    is_phishing = d[\"is_phishing\"]\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "Chat log (player messages only):\n",
        "---------------------------------------------------\n",
        "\\n{messages}\\n\\n\n",
        "---------------------------------------------------\n",
        "Compromised account analysis:\n",
        "---------------------------------------------------\n",
        "\\n{features}\\n\\n\n",
        "---------------------------------------------------\n",
        "\n",
        "Machine learning features:\n",
        "---------------------------------------------------\n",
        "\\n{ml_features}\\n\\n\n",
        "---------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "    ft_training_data_list.append(\n",
        "        {\n",
        "            \"instruction\": system_prompt,\n",
        "            \"input\": user_prompt,\n",
        "            # Keep the same output format your Tinker model expects.\n",
        "            \"output\": f\"is_phishing: {is_phishing}, agent_notes: {d.get('agent_notes')}\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Prepared examples:\", len(ft_training_data_list))\n",
        "\n",
        "# If you want to write, write to a VERSIONED path by default.\n",
        "out_path = Path(f\"../data/finetuning/training_data_{RUN_TAG}.json\")\n",
        "\n",
        "if WRITE_TRAINING_DATA:\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if out_path.exists() and not OVERWRITE_OUTPUTS:\n",
        "        raise FileExistsError(\n",
        "            f\"Refusing to overwrite existing file: {out_path}. Set OVERWRITE_OUTPUTS=True to overwrite.\"\n",
        "        )\n",
        "\n",
        "    import json\n",
        "\n",
        "    out_path.write_text(json.dumps(ft_training_data_list, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "    print(\"Wrote:\", out_path)\n",
        "else:\n",
        "    print(\"WRITE_TRAINING_DATA is False; not writing any files.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
